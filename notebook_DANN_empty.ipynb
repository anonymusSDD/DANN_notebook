{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gndOYv4sOgcI"
      },
      "source": [
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| Anonyme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "// *Version vide, sans les sorties* //"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io_m1A--488S"
      },
      "source": [
        "# Domain-Adversarial Training of Neural Networks\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note préliminaire** :\n",
        "Ce notebook a été rédigé sur **Visual Studio Code** et est donc plus agréable à lire sur cette plateforme (couleurs des Markdown etc.), le visuel constitue un élément pédagogique tout de même important. \n",
        "\n",
        "Cependant, veuillez noter que pour entrainer les réseaux de neurones, il est recommandé d'utiliser cuda (GPU), ou au minimum un CPU puissant (celui de ma machine ne suffit pas par exemple alors que le CPU de Google Colab est ok). \n",
        "\n",
        "Il est donc conseillé de :\n",
        "- prendre connaissance du notebook sur VScode ou directement sur le GitHub (anonyme !) suivant : https://github.com/anonymusSDD/DANN_notebook, \n",
        "- puis de faire tourner les exemples sur Google Colab ([en cliquant ici](https://colab.research.google.com/github/anonymusSDD/DANN_notebook/blob/main/notebook_DANN_empty.ipynb?hl=fr)).\n",
        "\n",
        "Bonne lecture !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_dylVg7OgcK"
      },
      "source": [
        "## Contenu du notebook\n",
        "\n",
        "Ce notebook présente une nouvelle approche d'Adaptation de Domaine en deep learning.\n",
        "\n",
        "0. [Préparation](#sec0)\n",
        "1. [Contexte : l'Adaptation de Domaine](#sec1)\n",
        "2. [Mathématiquement](#sec2)\n",
        "3. [Shallow DANN (version simple)](#sec3)\n",
        "4. [Généralisation à des architecture de réseaux “profonds”](#sec4)\n",
        "5. [Datasets : MNIST & MNIST-M](#sec5)\n",
        "6. [Test du DANN](#sec6)\n",
        "7. [Autres résultats](#sec7)\n",
        "8. [Ouverture à d'autres applications](#sec8)\n",
        "9. [Conclusion](#sec9)\n",
        "10. [Références](#sec10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfgaN24b58f1"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "**A la fin de ce notebook, vous devriez :**  \n",
        "- avoir compris les enjeux de l'<b>Adaptation de Domaine (DA)</b> ;\n",
        "- avoir découvert une nouvelle technique de DA appliqué aux réseaux de neurones : le <b>Domain-Adversarial Neural Network (DANN)</b> ;\n",
        "- savoir décrire l'architecture d'un DANN ;\n",
        "- connaitre les points forts et applications du DANN.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNu8CaueOgcL"
      },
      "source": [
        "# <a id=\"sec0\">0. Préparation</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsDrS6zlOgcL"
      },
      "source": [
        "Dans ce notebook, nous utiliserons `torch` et `torchvision`. Merci de se référer au site [PyTorch](https://pytorch.org/get-started/locally/) pour une installation en local (si choix de faire tourner en local). Nous utiliserons aussi les packages `sklearn`, `numpy`, et `matplotlib`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGVMIGIFOgcL"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "  <b>Exercice 1</b> :\n",
        "  Installer les packages nécessaires et vérifier que cela fonctionne en important tout.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLZxBJCXOgcL"
      },
      "outputs": [],
      "source": [
        "# !pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RNviKWfOgcM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import sklearn\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OppJNmyQOgcM"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "<b>Exercice 2</b> :\n",
        "Vérifier le type d'exécution.\n",
        "</div>\n",
        "\n",
        "Si exécution dans Google Colab, aller dans *Modifier le type d'exécution --> T4 GPU*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NUZFX_pGOgcM",
        "outputId": "cd948d55-047e-43da-aa21-97c07b01712c"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"No GPUs available.\")\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMwQsww2OgcN"
      },
      "source": [
        "## Mot-clés et définitions\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "    \n",
        "Avant de commencer, rappelons quelques mots clés en lien direct avec le sujet :\n",
        "\n",
        "- **Apprentissage profond** (Deep Learning) ;\n",
        "- **Réseaux de neurones** (Neural Networks - NN) ;\n",
        "- **Données synthétiques** (Synthetic Data) : données générées artificiellement, souvent à l'aide de techniques algorithmiques, qui peuvent être utilisées comme substitut aux données réelles. Elles sont utiles dans les situations où les données réelles sont limitées, coûteuses à obtenir, ou lorsqu'il y a des préoccupations en matière de confidentialité.\n",
        "- **Classification d'images** (Image Classification) ;\n",
        "- **Analyse de sentiments** (Sentiment Analysis) : technique utilisée en traitement du langage naturel pour identifier et classer les opinions ou sentiments exprimés dans un texte, généralement comme positif, négatif ou neutre. Elle est largement utilisée pour analyser les opinions des consommateurs dans les avis en ligne, les médias sociaux, etc.\n",
        "- **Adaptation de domaine** (Domain Adaptation) : notion clé de ce cours, il s'agit d'une technique en apprentissage automatique où un modèle est formé sur un domaine ou une distribution de données (par exemple, des images prises pendant la journée) et adapté pour fonctionner efficacement sur un domaine différent mais lié (par exemple, des images prises la nuit). Cette méthode est souvent utilisée pour surmonter le problème où les données sur lesquelles le modèle est testé diffèrent de celles sur lesquelles il a été entraîné.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZtwXht6OgcN"
      },
      "source": [
        "# <a id=\"sec1\">1. Contexte : l'Adaptation de Domaine</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdikYKcsOgcN"
      },
      "source": [
        "## Domain Adaptation (DA)\n",
        "\n",
        "![Schéma Domaine Adaptation](./img/schema_DA.png)\n",
        "\n",
        "(*source image : cours Philippe Giguère [5]*)\n",
        "\n",
        "Dans le contexte des réseaux de neurones, posséder suffisamment de données labélisées à entrainer est souvent un problème. Il est toujours possible d'obtenir des ensembles de données suffisamment grands en rajoutant des données qui ressemblent, mais qui souffrent, au moment du test, d'un \"décalage\" par rapport aux données réelles. \n",
        "\n",
        "On a aussi cité précédemment le cas parlant d'un modèle entrainé sur des photos prises de jour mais que l'on voudrait utiliser sur des photos prises de nuit. Un autre exemple serait dans le contexte de l’analyse de sentiments sur des critiques écrites : si nous avons en notre possession des données labelisées concernant des critiques de films mais que nous voulons classifier les critiques de livres, comment faire ? (*cf photo ci-dessous*)\n",
        "\n",
        "Au final, toutes ces situations témoignent de l'enjeu de ce qu’on appelle l'**Adaptation de Domaine** (DA).\n",
        "\n",
        "![Schéma Domaine Adaptation 2](./img/schema_DA_2.png)\n",
        "\n",
        "(*source image: cours Philippe Giguère [5]*)\n",
        "\n",
        "Autrement dit, l'Adaptation de Domaine intervient lors de l'apprentissage d'un classificateur ou d'un prédicteur en présence d'un décalage entre les distributions d'entraînement (**domaine source**) et de test (**domaine cible**).\n",
        "\n",
        "Plusieurs cas sont possibles :\n",
        "- le domaine cible est complètement sans label : **adaptation de domaine non-supervisée** ;\n",
        "- le domaine cible a quelques échantillons labelisés : **adaptation de domaine semi-supervisée**.\n",
        "\n",
        "Dans ce notebook, nous allons nous concentrer sur le cas le plus complexe et considérer le **domaine cible sans label**.\n",
        "Nous faisons tout de même la remarque que le cas semi-supervisé fonctionne de manière similaire une fois le principe compris sur le cas le plus difficile. Les échantillons labelisés (si existants) du domaine cible peuvent également servir à affiner le modèle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysx0J4biOgcO"
      },
      "source": [
        "## Domain Adaptation et Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw9A8R_xOgcO"
      },
      "source": [
        "Ces dernières années, le DA a vu ses performances largement augmenter avec la révolution du deep learning.\n",
        "\n",
        "Dans ce notebook, nous allons étudier en détail une théorie de DA en deep learning : le **Domain-Adversarial Neural Network (DANN)** qui s'inspire du principe du GAN (Generative Adversarial Nets). Il s'appuie notamment sur la théorie d'adaptation de domaine de Ben-David et al. (2006, 2010) [7, 8] et la combine avec du deep learning au sein d’un **seul processus d’entrainement**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpaikDwaOgcO"
      },
      "source": [
        "## Domain-Adversarial Neural Network (DANN)\n",
        "\n",
        "Nous allons, au cours de ce notebook, expliquer plusieurs fois et avec des approches différentes ce qu'est un DANN.\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "Le principe du <b>Domain-Adversarial Neural Network (DANN)</b> est basé sur l'apprentissage de caractéristiques qui sont à la fois bonnes pour effectuer une tâche d'apprentissage (comme la classification) et invariantes par rapport au changement de domaine (source vers cible). Cela est particulièrement important dans les scénarios où il y a un écart ou un décalage (shift) entre les données d'entrainement (domaine source) et les données sur lesquelles le modèle sera appliqué (domaine cible).\n",
        "</div>\n",
        "\n",
        "La particularité du DANN est qu'il permet l'apprentissage des caractéristiques, l'adaptation de domaine et la classification **au sein d'une seule architecture unifiée et utilisant un seul algorithme d’apprentissage** (backpropagation).\n",
        "\n",
        "Pour cela, nous avons besoin de deux classificateurs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQdI3ngnOgcO"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "<b>Exercice 3</b> : Quels sont, selon vous, les deux classificateurs et quelles sont leurs particularités ?\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQGkYVdDOgcO"
      },
      "source": [
        "<details class=\"alert alert-danger\">\n",
        "    <summary markdown=\"span\"><b>Voir la solution ? (cliquer ici)</b></summary>\n",
        "\n",
        "(i) un <b>classificateur/prédicteur de labels</b> utilisé pendant l’entrainement et pendant le test ;\n",
        "\n",
        "(ii) un <b>classificateur de domaine</b> qui distingue le domaine source du domaine cible pendant l’entrainement.\n",
        "\n",
        "Il faut, bien entendu, que le prédicteur ait un faible risque de se tromper lorqu'on le test sur les exemples sources.\n",
        "\n",
        "Rappelons que le but est d'apprendre un modèle qui peut bien se généraliser d'un domaine à l'autre, c'est à dire que le classificateur de domaine ne \"sache plus reconnaitre\" la source de la cible à la fin den l'entrainement. Nous voulons donc que le <b>classificateur de domaine se trompe</b> ! Cela va se faire par l'intégration d'une couche supplémentaire appelée <b>Gradient Reversal Layer (GRL)</b> et que nous détaillerons plus tard.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mJdDv7lOgcO"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "\n",
        " **Remarque importante :**\n",
        "\n",
        " D’habitude, les paramètres des classificateurs sont optimisés afin de minimiser les erreurs sur l’ensemble de l’apprentissage.\n",
        "\n",
        " Ici, les paramètres sont optimisés afin de **minimiser l’erreur du classificateur de labels** mais pour **maximiser l’erreur du classificateur de domaine**. La dernière étape travaille donc de manière antagoniste **«adversarially»** et encourage les caractéristiques invariantes des domaines à émerger au cours de l’optimisation.\n",
        "</div>\n",
        "\n",
        " Tout ce processus d’entrainement est regroupé au sein du **Domain-Adversarial Neural Network (DANN)** dont l'architecture générale est présentée ci-dessous :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozhDeU9XOgcP"
      },
      "source": [
        "![Architecture DANN](./img/DANN_architecture.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAg1XFo9OgcP"
      },
      "source": [
        "La légende explique une première fois le fonctionnement du DANN mais nous allons y revenir, en utilisant d'autres mots, et nous détaillerons pogressivement chaque étape de sa construction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0HlRX8VOgcP"
      },
      "source": [
        "La première remarque que nous pouvons faire est que nous reconnaissons une architecture classique d'un réseau de neurones avec :\n",
        "- un **feature extractor** (vert),\n",
        "- un **label predictor** (bleu),\n",
        "\n",
        "qui à eux deux forment une architecture feed-forward classique (couches et fonctions loss classiques). Ils sont entrainés en utilisant un algorithme de backpropagation basé sur une descente de gradient stochastique (ou des versions modifiées).\n",
        "\n",
        "La particularité du DANN est l'ajout du troisième composant : le **domain classifier** (rouge) associé à une couche spéciale appelée le **gradient reversal layer** (GRL). Cette dernière :\n",
        "- laisse les entrées inchangées lors de l’étape de forward propagation,\n",
        "- mais inverse le gradient en le multipliant par un scalaire négatif lors de la backpropagation.\n",
        "\n",
        "Finalement, on voit que toute architecture de réseaux de neurones classique entrainable par backpropagation peut servir de base à un DANN, ce qui le rend très pratique !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa7zvkgNOgcP"
      },
      "source": [
        "# <a id=\"sec2\">2. Mathématiquement </a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AVBfi2aOgcP"
      },
      "source": [
        "Revenons sur toutes les notions expliquées précédemment mais d'un point de vue mathématique cette fois-ci."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We7qiTV9OgcQ"
      },
      "source": [
        "## Domain Adaptation (DA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TTBSCr0OgcQ"
      },
      "source": [
        "On considère une tâche de classification où $X$ est l'expace d'entrée et $Y = \\{0,1,\\ldots,L-1\\}$ l'ensemble des $L$ labels possibles. Nous avons deux distributions :\n",
        "- la première sur $X \\times Y$, appelée *domaine source* $D_S$,\n",
        "- et le *domaine cible* $D_T$.\n",
        "\n",
        "Un algorithme d'apprentissage d'*adaption de domaine non-supervisée* est alors donné par un échantillon source labélisé $S$ i.i.d. de $D_S$, et d'un échantillon cible non-labélisé $T$ i.i.d. de $D^X_T$, où $D^X_T$ est la distribution marginale de $D_T$ par rapport à $X$.\n",
        "\n",
        "$$S = \\{(x_i, y_i)\\}_{i=1}^n \\sim (D_S)^n ; \\quad T = \\{x_i\\}_{i=n+1}^N \\sim (D^X_T)^{n'}$$\n",
        "\n",
        "avec $N = n + n'$ le nombre total d'échantillons. L'objectif de l'algorithme d'apprentissage est de construire un classificateur $$\\eta : X \\rightarrow Y$$ avec un faible *risque sur la cible*\n",
        "\n",
        "$$\n",
        "R_{D_T}(\\eta) = Pr_{(x,y)\\sim D_T} (\\eta(x) \\neq y),\n",
        "$$\n",
        "tout en ayant aucune information sur les labels de $D_T$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKW35T3yOgcQ"
      },
      "source": [
        "## Divergence de domaine\n",
        "\n",
        "L'implémentation d'un DANN demande l'utilisation d'une métrique permettant de mesurer la divergence entre les domaines. La théorie décrite ici applique la **$H$-divergence** présentée par Ben-David et al. (2006, 2010) [7, 8].\n",
        "\n",
        "![Définition 1](./img/def1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "outQ92Q1OgcQ"
      },
      "source": [
        "### Proxy Distance (PAD)\n",
        "\n",
        "Il est généralement compliqué de calculer exactement cette distance $\\hat{d}_{\\mathcal{H}}(S, T)$ mais nous pouvons l'approximer relativement facilement. Pour le faire, nous avons besoin d'un nouveau dataset :\n",
        "\n",
        "(2)\n",
        "$$    \n",
        "U = \\{ (x_i, 0) \\}^n_{i=1} \\cup \\{ (x_i, 1) \\}^N_{i=n+1},\n",
        "$$\n",
        "\n",
        "où les exemples de l'échantillon source sont labélisés 0 et les exemples de l'échantillon cible sont labélisés 1.\n",
        "Le risque du classificateur entrainé sur le nouveau dataset $U$ approxime le terme \"min\" de l'Equation (1). Etant donné une erreur générale $\\epsilon$ sur la problème de discrimination entre source et cible, la $H$-divergence est ensuite approximée par\n",
        "\n",
        "(3)\n",
        "$$\n",
        "\\tilde{d}_{A} = 2(1 - 2\\varepsilon).         \n",
        "$$\n",
        "\n",
        "Ben-David et al. (2006) appellent cette valeur $\\tilde{d}_{A}$ la *Proxy A-distance* (PAD). La A-distance étant définie comme $d_{A}(D_S^X, D_T^X) = 2 \\sup_{A \\in \\mathcal{A}} | \\text{Pr}_{D_S^X}(A) - \\text{Pr}_{D_T^X}(A) |$ , où $A$ est un sous-ensemble de $X$. Notons qu'en choisissant $A = \\{ A_n | n \\in \\mathcal{H} \\}$, avec $A_n$ l'ensemble représenté par la fonction caractéristiques $\\eta$, la A-distance et la $H$-divergence de la Définition 1 sont identiques.\n",
        "\n",
        "Dit de manière simple, la PAD est une métrique estimant la similarité des représentations source et cible. Pour l'obtenir voici les étapes à suivre : \n",
        "- construire l'ensemble de données U de l'Équation (2) en utilisant à la fois les représentations sources et cibles des échantillons d'apprentissage ; \n",
        "- diviser aléatoirement U en deux sous-ensembles de taille égale ; \n",
        "- entraîner des SVM linéaires sur le premier sous-ensemble de U en utilisant une large gamme de valeurs de C (hyper-paramètre du SVM) ; \n",
        "- calculer l'erreur de tous les classificateurs obtenus sur le second sous-ensemble de U ;\n",
        "- utiliser l'erreur la plus faible pour calculer la valeur de PAD de l'Équation (3).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Sh34fMXqLE"
      },
      "source": [
        "Pour plus de précisions sur la théorie du DANN, et plus précisément sur comment diminuer le risque sur le domaine cible et contrôler la $H$-divergence est mathématiquement lié au fait que le domaine source et cibles soient indiscernables, voir l'article [Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818) [1] pages 6 et 7, théorème 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7OlJV9vOgcd"
      },
      "source": [
        "# <a id=\"sec3\">3. Shallow DANN (version simple)</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U12vLGAvOgcd"
      },
      "source": [
        "### Algorithme\n",
        "\n",
        "La première étape avant de construire notre DANN est de proposer une approche simple pour incorporer un composant de DA dans un réseau de neurones. Le plus simple que nous puissions faire est le réseau suivant :\n",
        "- une seule couche cachée,\n",
        "- et fully connected.\n",
        "\n",
        "C'est ce que nous allons appeler dans la suite un **Shallow DANN** et voici le pseudo-code correspondant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEGwndB1Ogcd"
      },
      "source": [
        "![ALgorithme 1 - Shallow DANN](./img/algo1.png)\n",
        "\n",
        "Ici est fait le choix de passer sous silence la démonstration mathématique liée à cet algorithme. Pour les plus curieux, vous pouvez la retrouver aux pages 7-10 pour le Shallow DANN (et 11-13 pour la version généralisée) de l'article [Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818) [1].\n",
        "\n",
        "Nous revenons tout de même sur les principes fondamentaux.\n",
        "\n",
        "<div class=\"alert alert-warning\">\n",
        "Rappelons que l’optimisation implique la minimisation de certains paramètres et la maximisation d’autres.\n",
        "</div>\n",
        "\n",
        "Il est proposé d'aborder le problème avec une procédure simple de gradient stochastique, dans laquelle **les mises à jour sont effectuées dans le sens opposé du gradient pour les paramètres minimisant**, et **dans le sens du gradient pour les paramètres maximisants**.\n",
        "L'algorithme 1 fournit le pseudo-code complet de cette procédure d'apprentissage :\n",
        "- un couche cachée $G_{f}(.)$ qui mappe toutes les données (source ou cible) ;\n",
        "- une couche de sortie $G_{y}(.)$ qui classe avec précision les échantillons source ;\n",
        "- cette dernière couche paralyse en même temps la capacité du régresseur de domaine $G_{d}(.)$ à détecter si chaque exemple provient du domaine source ou cible ;\n",
        "- le réseau de neurones [feature extractor (vert) + label predictor (bleu)] est paramétré par $W, b, V, c$ ;\n",
        "- le régresseur de domaine [rouge] est paramétré par $u$ et $e$ ;\n",
        "- les paramètres du réseau de neurone sont actualisés dans le sens inverse du gradient (signe \"-\" dans le pseudo-code) ;\n",
        "- les paramètres du régresseur de domaine sont actualisés dans le sens du gradient (signe \"+\" dans le pseudo-code).\n",
        "\n",
        "\n",
        "<div class=\"alert alert-warning\">\n",
        "<b>Exercice 4</b> : Maintenant que tu comprends un peu mieux le fonctionnement du DANN, peux-tu expliquer avec tes propres mots l'origine de son nom ?\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgeQZdgBOgcd"
      },
      "source": [
        "<details class=\"alert alert-danger\">\n",
        "    <summary markdown=\"span\"><b>Voir la solution ? (cliquer ici)</b></summary>\n",
        "\n",
        "Lors de l'entraînement, le réseau de neurones et le régresseur se font concurrence, de manière contradictoire.\n",
        "C'est l'explication au terme <b>\"Adversarial\"</b> dans Domain-Adversarial Neural Networks (DANN).\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bepugI9GXqLP"
      },
      "source": [
        "### Exemple : problème des *inter-twinning moons*\n",
        "\n",
        "Avec cette version simplifiée du DANN (que nous n'implémenterons pas), il est possible fournir des résultats déjà très bons. Plusieurs résultats intéressants sont disponibles dans l'article [Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818) [1] aux pages 13-19.\n",
        "\n",
        "Pour illustrer ce cours, il a été choisi de présenter les résultats d'un seul problème : le problème 2D des *inter-twinning moons* composé de deux ensembles de points qui forment deux formes en demi-cercle (\"lunes\"), qui se chevauchent partiellement (d'où le terme \"inter-twinning\"). Chaque lune représente une classe différente.\n",
        "\n",
        "![Toy problem - shallow DANN](./img/toy_problem.png)\n",
        "\n",
        "Conditions de l'expérience :\n",
        "1. Un **échantillon source S** comprenant 150 exemples issus de la lune inférieure (label 0) et 150 exemples issus de la lune supérieure (label 1) --> {points rouges et verts} ;\n",
        "2. Un **échantillon cible T** de 300 exemples non labelisés obtenu par la procédure suivante {points noirs} :\n",
        "    - génération d'un échantillon S' de la même manière que S a été généré ;\n",
        "    - rotation de chaque exemple de 35° ;\n",
        "3. Un **DANN** VS un **réseau de neurones classique (NN)**, avec la même architecture de base (une couche cachée de 15 neurones) ;\n",
        "4. NN entraîné exactement comme le DANN : mise à jour du régresseur de domaine grâce à T (avec un hyper-paramètre λ = 6 ; même valeur que pour DANN), mais la backpropagation adversariale est désactivée dans la couche cachée (cf algorithme 1 en omettant les lignes 22-31).\n",
        "\n",
        "Analysons les résultats colonne par colonne :\n",
        "\n",
        "- **Label classification** : on voit que le NN classe correctement les deux classes de l'échantillon source S, mais n'est pas entièrement adapté à l'échantillon cible T. Avec l'application du DANN, la frontière de décision est légèrement déplacée ce qui fait que les points du domaine cible tombent du bon côté et donc ce qui augmente le performance du modèle. L'exemple du DANN classe parfaitement les exemples des échantillons source et cible.\n",
        "- **Representation PCA** : les graphiques sont obtenus en appliquant une analyse en composantes principales (PCA) sur l'ensemble de toutes les données source et cible. Dans la représentation PCA-DANN, les points cibles sont homogènement répartis parmi les points sources. Dans la représentation PCA-NN, les points cibles appartiennent clairement à des clusters ne contenant aucun point source. Labéliser les points cibles semble être une tâche clairement plus facile pour le DANN. Notons aussi les quatre lettres A, B, C et D, qui correspondent aux quatre extrêmes de l'espace original. Nous observons que les points A et B (de même pour C et D) sont très proches l'un de l'autre dans la représentation PCA-NN, alors qu'ils appartiennent clairement à des classes différentes à l'origine. Au contraire, la représentation PCA-DANN respecte bien cette séparation et est une nouvelle fois mieux adaptée au problème d'adaptation.\n",
        "- **Domain classification** : ici, on visualise la frontière de décision sur le problème de classification de domaine. L'interprétation est légèrement plus subtile. Le NN arrive plus ou moins à séparer les éléments venant du domaine source de ceux du domaine cible (frontière grise), certes de manière très imparfaite, mais on devine la tendance à suivre la rotation de la courbe. Au contraine, le DANN échoue complètement ce qui témoigne de sa capacité à généraliser les données en rendant indiscernable le domaine source du domaine cible. Le DANN a donc le comportement attendu.\n",
        "- **Hidden neurons** : le même constat qu'au-dessus peut s'appliquer. Alors que les 15 lignes du NN sont globalement capables de capturer l'angle de rotation du problème de classification de domaine en se regroupant en deux motifs prédominants (deux lignes parallèles traversant le cadre d'en bas à gauche jusqu'en haut à droite), elles disparaissent complètement dans le cas du DANN.\n",
        "\n",
        "**Conclusion** : le shallow DANN fonctionne très bien sur des exemples simples !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remarque : le choix des hyper-paramètres (paramètre d'adaptation $\\lambda$, taux d'apprentissage $\\mu$, taille couche cachée ... ) se fait par la méthode *reverse-cross validation* proposée par Zhong et al. (2010) [11]. Pour plus d'informations, cf page 16 de l'article [Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818) [1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zPcqN5IOgce"
      },
      "source": [
        "# <a id=\"sec4\">4. Généralisation à des architecture de réseaux “profonds”</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r0Lv8CTOgce"
      },
      "source": [
        "# Architectures arbitraires\n",
        "\n",
        "Nous allons maintenant nous appuyer sur la section précédent pour généraliser le DANN à des architectures arbitraires, plus appropriées pour de nombreuses applications.\n",
        "\n",
        " Rappelons l'architecture générale du DANN :\n",
        "\n",
        " ![Architecture DANN](./img/DANN_architecture_red.png)\n",
        "\n",
        "Dans la version générale, nous introduison un couche spéciale appelée **gradient reversal layer (GRL)** entre le feature extracteur (vert) et le regresseur de domaine (rouge) :\n",
        "- la GRL ne nécessite aucun paramètre ;\n",
        "- pendant la **forward propagation**, la GRL agit de manière totalement **transparente** (= transformation identité) ;\n",
        "- pendant la **backpropagation**, la GRL prend le gradient du niveau actuel et **change son signe** en multipliant tout simplement par le scalaire -1, avant de passer à la couche précédente et de remonter tout le réseau.\n",
        "\n",
        "On revient une fois nouvelle sur le terme \"adversarial\". En effet, l'entraînement du DANN dualise deux objectifs :\n",
        "- **Apprentissage de la tâche principale** : le feature extracteur (vert) et le prédicteur/classificateur de labels (bleu) sont entraînés ensemble sur les données du domaine source ;\n",
        "- **Adversarial Training** : le feature extracteur (vert) est également utilisé pour alimenter le regresseur de domaine (rouge). L'objectif est d'empêcher le regresseur de domaine de distinguer correctement les échantillons provenant du domaine source de ceux du domaine cible. Cela est réalisé en utilisant la \"Gradient Reversal Layer\" (GRL) qui encourage le feature extracteur (vert) à apprendre des caractéristiques qui sont utiles pour la tâche principale mais qui ne permettent pas la discrimination de domaine.\n",
        "\n",
        "*Remarque* : après l’apprentissage, le réseau peut bien sûr être utilisé pour prédire des labels pour des échantillons provenant du domaine cible (qui n’ont pas de labels) mais aussi provenant du domaine source (bonne précision normalement)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rgFu4zBOgce"
      },
      "source": [
        "## Implémentation DANN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqSqQ8QOOgce"
      },
      "source": [
        "Nous allons suivre l'architecture et la méthode détaillées dans l'article [Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818) [1].\n",
        "\n",
        "Le code est inspiré de [NaJaeMin93](https://github.com/NaJaeMin92/pytorch-DANN/blob/master) [3] et adapté pour le besoin de ce cours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-iBZEUXOgce"
      },
      "source": [
        " ![Architecture DANN appliquée à MNIST](./img/MNIST_NN_architecture.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ReXf1opOgcf"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qvbrEhKOgcf"
      },
      "source": [
        "### 1. Encodeur - feature extracteur (vert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHsHZIaaXqLQ"
      },
      "source": [
        "Le **feature extractor ou encodeur** est la partie du réseau qui apprend à extraire des caractéristiques (features) pertinentes à partir des données d'entrée (source et cible). Ces caractéristiques sont utilisées à la fois pour la tâche de classification principale et pour la tâche de discrimination de domaine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ46DrPkOgcf"
      },
      "outputs": [],
      "source": [
        "class Extractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Extractor, self).__init__()\n",
        "        self.extractor = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=48, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.extractor(x)\n",
        "        x = x.view(-1, 3 * 28 * 28)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU_J6r8wOgcf"
      },
      "source": [
        "### 2. Classificateur - prédicteur de labels (bleu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUYYfAm3XqLQ"
      },
      "source": [
        "Le **classificateur ou prédicteur de labels** forme avec l'extracteur un réseau de neurones classique qui utilise les caractéristiques extraites pour effectuer la tâche principale (comme la classification ou la régression) sur le domaine source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9zC5I29Ogcf"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=3 * 28 * 28, out_features=100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=100, out_features=100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=100, out_features=10),\n",
        "            # pas besoin du softmax à la fin car déjà inclus dans la loss nn.CrossEntropyLoss\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8xraoCOOgcg"
      },
      "source": [
        "### 3. Discriminateur - régresseur de domaine (rouge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhVSi4MxXqLR"
      },
      "source": [
        "Le **discriminateur ou régresseur de domaine** est un autre réseau de neurones qui tente de déterminer si les caractéristiques extraites proviennent du domaine source ou cible.\n",
        "Une architecture simple (x->100->2) est utilisée afin d'accélérer les tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b7kLZg4Ogcg"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "# Fonction pour inverser le gradient\n",
        "class ReverseLayerF(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTwUv0S0Ogcg"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.discriminator = nn.Sequential(\n",
        "            nn.Linear(in_features=3 * 28 * 28, out_features=100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=100, out_features=2),\n",
        "            # pas besoin de sigmoid car déjà inclus dans la loss nn.CrossEntropyLoss\n",
        "        )\n",
        "\n",
        "    def forward(self, input_feature, alpha):\n",
        "        reversed_input = ReverseLayerF.apply(input_feature, alpha)\n",
        "        x = self.discriminator(reversed_input)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXFd_r93OgcR"
      },
      "source": [
        "# <a id=\"sec5\">5. Datasets : MNIST & MNIST-M</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-40en2zOgcR"
      },
      "source": [
        "Si nous voulons construire notre propre DANN, nous avons besoin d'un jeu de données source et d'un jeu de données cible adaptés au contexte d'étude.\n",
        "\n",
        "Dans ce notebook, il a été fait le choix de manipuler le célèbre **dataset MNIST** (LeCun et al., 1998) [10]. Il sera notre **dataset source**. Pour rendre ce cours plus intéractif, nous allons construire nous même notre dataset cible : **MNIST-M**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgECuAzoOgcR"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "Le <b>dataset MNIST-M</b> est une variante du dataset MNIST, largement utilisé dans le domaine de l'apprentissage automatique, de la reconnaissance de caractères manuscrits et bien entendu en adaptation de domaine. Alors que le dataset MNIST original contient des images en niveaux de gris de chiffres écrits à la main (de 0 à 9), MNIST-M introduit une complexité supplémentaire : les chiffres du dataset MNIST original sont fusionnés avec des patches colorés et aléatoires provenant de photos. Cela crée un effet de bruit de fond coloré, rendant la reconnaissance des chiffres plus difficile.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y19U8nloOgcR"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "\n",
        "  Pour résumer, nous allons donc avoir :\n",
        "\n",
        "- un domaine source : dataset MNIST,\n",
        "- un domaine cible : dataset MNIST-M.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9jCX6JHOgcR"
      },
      "outputs": [],
      "source": [
        "# Quelques fonctions utiles pour la suite\n",
        "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
        "import torch.utils.data as data\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYuyJ26nOgcS"
      },
      "source": [
        "### Paramètres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDuQ5V0DOgcS"
      },
      "outputs": [],
      "source": [
        "# Définition des paramètres\n",
        "batch_size = 32 # valeur adaptée à notre exemple\n",
        "epochs = 3 # 3 époques pour commencer, on pourra augmenter ce nombre si besoin, attention au temps de calcul\n",
        "num_workers = 4 # nombre de workers pour le chargement des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk8kvUvsOgca"
      },
      "source": [
        "### MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9OkOD0_eOgca",
        "outputId": "e078beb8-87be-4775-b40e-7df9da8b600c"
      },
      "outputs": [],
      "source": [
        "# Chargement du dataset MNIST avec PyTorch\n",
        "transform = transforms.Compose([transforms.ToTensor(), # Transformation des images en tensors PyTorch\n",
        "                                transforms.Normalize((0.1307,), (0.3081,)) # Valeurs de normalisation connues pour le dataset MNIST\n",
        "                                ])\n",
        "\n",
        "mnist_train_dataset = datasets.MNIST(root='./data/MNIST', train=True, download=True, transform=transform)\n",
        "mnist_valid_dataset = datasets.MNIST(root='./data/MNIST', train=True, download=True, transform=transform)\n",
        "mnist_test_dataset = datasets.MNIST(root='./data/MNIST', train=False, transform=transform)\n",
        "\n",
        "indices = list(range(len(mnist_train_dataset)))\n",
        "validation_size = 5000\n",
        "train_idx, valid_idx = indices[validation_size:], indices[:validation_size]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "\n",
        "mnist_train_loader = DataLoader(\n",
        "    mnist_train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    sampler=train_sampler,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "mnist_test_loader = DataLoader(\n",
        "    mnist_test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "tFo3LzSkOgcb",
        "outputId": "c85c3cea-2ddc-4506-a905-ee8cd275800a"
      },
      "outputs": [],
      "source": [
        "#Visualisation de quelques images issues du dataset MNIST avec leurs labels, en noir et blanc\n",
        "\n",
        "print(\"------ Exemples d'images du dataset MNIST ------\")\n",
        "plt.figure(figsize=(16, 6))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    image_tensor, label = mnist_train_loader.dataset[i]\n",
        "    image_numpy = image_tensor.permute(1, 2, 0).numpy()  # Convertit le tensor en format numpy et réarrange les dimensions\n",
        "    plt.imshow(image_numpy, cmap=\"gray\")\n",
        "    plt.title(f\"Label: {label}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1k1ZcH9Ogcb"
      },
      "source": [
        "### MNIST-M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWxa-1LTOgcb"
      },
      "source": [
        "Il n'y a pas de dataset MNIST-M disponible dans les librairies PyTorch.\n",
        "Au lieu d'importer ces données d'une autre source, nous allons plutôt créer notre propre dataset personnalisé.\n",
        "Pour cela, il va vous falloir télécharger une photo. Il vous est proposé d'utiliser celle qui a été choisie pour cet exemple *(./img/background.jpg ou ./img/background2.jpg)* mais il vous est également possible de charger une photo de votre choix dans la cellule ci-dessous. Pensez à choisir une image avec de belles couleurs vives pour de plus beaux résultats.\n",
        "\n",
        "Voici un lien où vous pouvez trouver des photos libres de droit : [site web photos](https://pixabay.com/fr/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "PqE3ay5WOgcb",
        "outputId": "646d40c4-00e5-44fb-d6ef-ad2da290a871"
      },
      "outputs": [],
      "source": [
        "# Chargement de l'image qui sera utilisée en fond d'écran, au choix\n",
        "\n",
        "background_img = Image.open('./img/background.jpg')\n",
        "# background_img = Image.open('./img/background2.jpg')\n",
        "\n",
        "# Vérifier que les dimensions de l'image sont plus grandes que 28x28 pixels\n",
        "print(background_img.size)\n",
        "\n",
        "# Afficher l'image téléchargée\n",
        "plt.imshow(background_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OU si utilisation de **Google Colab** (enlever les commentaires) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Téléchargement du dossier DANN depuis le dépôt GitHub\n",
        "# !git clone https://github.com/anonymusSDD/DANN_notebook.git\n",
        "\n",
        "# # Chargement de l'image qui sera utilisée en fond d'écran, au choix\n",
        "\n",
        "# background_img = Image.open('./DANN_notebook/img/background.jpg')\n",
        "# # background_img = Image.open('./DANN_notebook/img/background2.jpg')\n",
        "\n",
        "# # Vérifier que les dimensions de l'image sont plus grandes que 28x28 pixels\n",
        "# print(background_img.size)\n",
        "\n",
        "# # Afficher l'image téléchargée\n",
        "# plt.imshow(background_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHj-Yh1YOgcb"
      },
      "outputs": [],
      "source": [
        "# Code pour importer une nouvelle fois le dataset MNIST, mais cette fois-ci en RGB\n",
        "\n",
        "# Transformation pour convertir les images MNIST en RGB\n",
        "def to_rgb(image):\n",
        "    rgb_image = Image.new(\"RGB\", image.size)\n",
        "    rgb_image.paste(image)\n",
        "    return rgb_image\n",
        "\n",
        "# Transformation pour convertir les images en tensors PyTorch, en RGB, mais pas de normalisation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(to_rgb),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Téléchargement de dataset MNIST en RGB\n",
        "mnist_dataset_train = datasets.MNIST(root='./data/MNIST', train=True, download=True, transform=transform)\n",
        "mnist_dataset_test = datasets.MNIST(root='./data/MNIST', train=False, download=True, transform=transform)\n",
        "\n",
        "# DataLoader\n",
        "mnist_train_loader2 = DataLoader(mnist_dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "iWk4GbdbOgcc",
        "outputId": "9827973c-7cc0-416e-cc41-c2af098a391f"
      },
      "outputs": [],
      "source": [
        "# Mixage des images MNIST avec le fond d'écran téléchargé\n",
        "\n",
        "# Fonction qui extrait des patchs aléatoires de l'image téléchargée\n",
        "def extract_random_patch(background, patch_size=(28, 28)):\n",
        "    width, height = background.size\n",
        "    x = random.randint(0, width - patch_size[0])\n",
        "    y = random.randint(0, height - patch_size[1])\n",
        "    return background.crop((x, y, x + patch_size[0], y + patch_size[1]))\n",
        "\n",
        "# Affichage d'un crop exemple\n",
        "print(\"----------- Crop exemple -----------\\n\")\n",
        "plt.imshow(extract_random_patch(background_img, patch_size=(28, 28)))\n",
        "plt.show()\n",
        "\n",
        "# Fonction qui mélange les chiffres de MNIST avec les patchs aléatoires\n",
        "def blend_mnist_digit_with_patch(mnist_digit, patch):\n",
        "    # Les chiffres MNIST doivent être au format PIL Image\n",
        "    mnist_digit = transforms.ToPILImage()(mnist_digit)\n",
        "\n",
        "    # Création d'un masque qui récupère la forme du chiffre des images MNIST\n",
        "    mnist_mask = mnist_digit.convert(\"L\").point(lambda x: 255 if x > 0 else 0, mode='1')\n",
        "\n",
        "    # Fusion du masque MNIST avec les patchs\n",
        "    patch_with_digit = Image.composite(mnist_digit.convert(\"RGB\"), patch, mnist_mask)\n",
        "\n",
        "    return patch_with_digit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kZLIu9NOgcc"
      },
      "outputs": [],
      "source": [
        "# Création d'une classe spécifique MNIST-M avec le fond d'écran personnalisé\n",
        "class CustomMNISTMDataset(data.Dataset):\n",
        "    def __init__(self, mnist_data, background, normalize, mean, std):\n",
        "        self.mnist_data = mnist_data\n",
        "        self.background = background\n",
        "        if normalize:\n",
        "          self.transform = transforms.Compose([\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize(mean, std)\n",
        "          ])\n",
        "        else:\n",
        "          self.transform = transforms.ToTensor()  # Ajout de la transformation pour convertir les images PIL en tensors\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mnist_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mnist_image, label = self.mnist_data[idx]\n",
        "        patch = extract_random_patch(self.background)\n",
        "        blended_image = blend_mnist_digit_with_patch(mnist_image, patch)\n",
        "        blended_image_tensor = self.transform(blended_image)  # Convertit l'image PIL en tensor\n",
        "        return blended_image_tensor, label\n",
        "\n",
        "# Calcule la moyenne et la variance pour normaliser les tensors ensuite\n",
        "def calculate_mean_std(loader):\n",
        "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
        "\n",
        "    for data, _ in loader:\n",
        "        # Redimensionner le batch (batch, channels, height, width) en (batch, channels, height*width)\n",
        "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
        "        channels_squared_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
        "        num_batches += 1\n",
        "\n",
        "    # Calculer la moyenne et l'écart-type\n",
        "    mean = channels_sum / num_batches\n",
        "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "# Création du dataset personnalisé MNIST-M AVANT normalisation pour calculer ses statistiques\n",
        "mnistm_dataset = CustomMNISTMDataset(mnist_dataset_train, background_img, False, 0, 0)\n",
        "mnistm_loader = DataLoader(mnistm_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEw9VriQOgcc"
      },
      "source": [
        "*La partie suivante peut mettre quelques secondes à tourner (surtout sur CPU), le calcul des statistiques est coûteux.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "LNuIMVL4Ogcc",
        "outputId": "0f911cb1-12ea-4747-d8db-082fc08c0d56"
      },
      "outputs": [],
      "source": [
        "# Calcul de la moyenne et de l'écart-type de notre nouveau dataset MNIST-M\n",
        "mean, std = calculate_mean_std(mnistm_loader)\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Std: {std}\")\n",
        "\n",
        "# Affichage du dataset MNIST-M personnalisé avant normalisation\n",
        "print(\"\\n----------- Exemples d'images issues du dataset MNIST-M avant normalisation -----------\\n\")\n",
        "plt.figure(figsize=(16, 6))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    image_tensor, label = mnistm_loader.dataset[i]\n",
        "    image_numpy = image_tensor.permute(1, 2, 0).numpy()  # Convertit le tensor en format numpy et réarrange les dimensions\n",
        "    plt.imshow(image_numpy)\n",
        "    plt.title(f\"Label: {label}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "OH6x1-j2Ogcc",
        "outputId": "56f05093-f1c6-4023-a634-abfc5bff614b"
      },
      "outputs": [],
      "source": [
        "# Création du dataset MNIST-M personnalisé AVEC normalisation\n",
        "mnistm_train_dataset_full = CustomMNISTMDataset(mnist_dataset_train, background_img, True, mean, std)\n",
        "mnistm_test_dataset = CustomMNISTMDataset(mnist_dataset_test, background_img, True, mean, std)\n",
        "\n",
        "indices = list(range(len(mnistm_train_dataset_full)))\n",
        "validation_size = 5000\n",
        "train_idx, valid_idx = indices[validation_size:], indices[:validation_size]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "\n",
        "# Création de DataLoaders pour le dataset MNIST-M personnalisé\n",
        "mnistm_train_loader = DataLoader(\n",
        "    mnistm_train_dataset_full,\n",
        "    batch_size=batch_size,\n",
        "    sampler=train_sampler,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "mnistm_test_loader = DataLoader(\n",
        "    mnistm_test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "# Affichage du dataset MNIST-M personnalisé APRES normalisation\n",
        "print(\"\\n----------- Exemples d'images issues du dataset MNIST-M après normalisation -----------\\n\")\n",
        "plt.figure(figsize=(16, 6))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    image_tensor, label = mnistm_train_loader.dataset[i]\n",
        "    image_numpy = image_tensor.permute(1, 2, 0).numpy()  # Convertit le tensor en format numpy et réarrange les dimensions\n",
        "    # Re-scale les données si elles sont en dehors de la plage [0, 1]\n",
        "    if image_numpy.dtype == np.float32 or image_numpy.dtype == np.float64:\n",
        "        image_numpy = np.clip(image_numpy, 0, 1)  # Assurez-vous que les valeurs sont entre 0 et 1\n",
        "    # Ou convertit en entiers si les valeurs sont dans la plage [0, 255]\n",
        "    elif image_numpy.max() > 1:\n",
        "        image_numpy = (image_numpy * 255).astype(np.uint8)\n",
        "    plt.imshow(image_numpy)\n",
        "    plt.title(f\"Label: {label}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn1U3DdTOgcd"
      },
      "source": [
        "Et voilà, nous avons nos deux datasets source et cible prêts à être entrainés avec un DANN !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkbU-LpbOgcg"
      },
      "source": [
        "# <a id=\"sec6\">6. Test du DANN </a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JUwUpBnqOgch",
        "outputId": "d6d7b937-62ae-4406-d4aa-fb470b73a0e5"
      },
      "outputs": [],
      "source": [
        "# Chargement des datasets MNIST (source) et MNIST-M (cible)\n",
        "\n",
        "# Source : 0, Target :1\n",
        "source_train_loader = mnist_train_loader\n",
        "target_train_loader = mnistm_train_loader\n",
        "\n",
        "source_test_loader = mnist_train_loader\n",
        "target_test_loader = mnistm_train_loader\n",
        "\n",
        "# Vérification que les dimensions sont compatibles avec notre NN\n",
        "print(\"------ Dimensions des images du dataset MNIST ------\")\n",
        "for i, (images, labels) in enumerate(source_train_loader):\n",
        "    print(f\"Batch {i}: Image Shape = {images.shape}\")\n",
        "    if i == 3:  # Affiche les dimensions des images des 3 premiers lots\n",
        "        break\n",
        "# Vérifier la dimension d'une image spécifique\n",
        "specific_image, _ = next(iter(source_train_loader))\n",
        "print(f\"Shape of a specific image: {specific_image[0].shape}\")\n",
        "\n",
        "# Si pas de GPU, cette partie peut bloquer et mettre beaucoup trop de temps (la commenter dans ce cas)\n",
        "\n",
        "print(\"\\n------ Dimensions des images du dataset MNIST-M ------\")\n",
        "for i, (images, labels) in enumerate(target_train_loader):\n",
        "    print(f\"Batch {i}: Image Shape = {images.shape}\")\n",
        "    if i == 3:  # Affiche les dimensions des images des 3 premiers lots\n",
        "        break\n",
        "specific_image, _ = next(iter(target_train_loader))\n",
        "print(f\"Shape of a specific image: {specific_image[0].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ypFb--w_Ogch",
        "outputId": "5f8513ec-7a30-4a1e-ec9f-97103cb12a0f"
      },
      "outputs": [],
      "source": [
        "# Utilisation du GPU si disponible\n",
        "if torch.cuda.is_available():\n",
        "    encoder = Extractor().cuda()\n",
        "    classifier = Classifier().cuda()\n",
        "    discriminator = Discriminator().cuda()\n",
        "else:\n",
        "    print(\"No GPUs available, the running time will long ... very\")\n",
        "    encoder = Extractor().cpu()\n",
        "    classifier = Classifier().cpu()\n",
        "    discriminator = Discriminator().cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fsWqPAxOgch"
      },
      "source": [
        "##### Quelques fonction utiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxC-UZj4Ogch"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import itertools\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def set_model_mode(mode='train', models=None):\n",
        "    for model in models:\n",
        "        if mode == 'train':\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "def optimizer_scheduler(optimizer, p):\n",
        "    \"\"\"\n",
        "    Adjust the learning rate of optimizer\n",
        "    :param optimizer: optimizer for updating parameters\n",
        "    :param p: a variable for adjusting learning rate\n",
        "    :return: optimizer\n",
        "    \"\"\"\n",
        "    # taux d'apprentissage ajusté durant la descente stochastique de gradient\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = 0.01 / (1. + 10 * p) ** 0.75 # formule donnée page 21 du papier [1]\n",
        "    return optimizer\n",
        "\n",
        "def save_model(encoder, classifier, discriminator, training_mode):\n",
        "    print('Saving models ...')\n",
        "    save_folder = 'trained_models'\n",
        "    if not os.path.exists(save_folder):\n",
        "        os.makedirs(save_folder)\n",
        "\n",
        "    torch.save(encoder.state_dict(), 'trained_models/encoder_' + str(training_mode) + '.pt')\n",
        "    torch.save(classifier.state_dict(), 'trained_models/classifier_' + str(training_mode) + '.pt')\n",
        "\n",
        "    if training_mode == 'dann':\n",
        "        torch.save(discriminator.state_dict(), 'trained_models/discriminator_' + str(training_mode) + '.pt')\n",
        "\n",
        "    print('The model has been successfully saved!')\n",
        "\n",
        "def plot_embedding(X, y, d, training_mode):\n",
        "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
        "    X = (X - x_min) / (x_max - x_min)\n",
        "    y = list(itertools.chain.from_iterable(y))\n",
        "    y = np.asarray(y)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(len(d)):  # X.shape[0] : 1024\n",
        "        # affiche chiffres colorés\n",
        "        if d[i] == 0:\n",
        "            colors = (0.0, 0.0, 1.0, 1.0)\n",
        "        else:\n",
        "            colors = (1.0, 0.0, 0.0, 1.0)\n",
        "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
        "                 color=colors,\n",
        "                 fontdict={'weight': 'bold', 'size': 9})\n",
        "\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "\n",
        "    save_folder = 'saved_plot'\n",
        "    if not os.path.exists(save_folder):\n",
        "        os.makedirs(save_folder)\n",
        "\n",
        "    fig_name = 'saved_plot/' + str(training_mode) + '.png'\n",
        "    plt.savefig(fig_name)\n",
        "    print('{} has been successfully saved!'.format(fig_name))\n",
        "\n",
        "def visualize(encoder, training_mode):\n",
        "    # Dessine 512 échantillons de test_data\n",
        "    source_test_loader = mnist_test_loader\n",
        "    target_test_loader = mnistm_test_loader\n",
        "\n",
        "    # Récupère échantillons source_test\n",
        "    source_label_list = []\n",
        "    source_img_list = []\n",
        "    for i, test_data in enumerate(source_test_loader):\n",
        "        if i >= 16:  # avoir seulement 512 échantillons\n",
        "            break\n",
        "        img, label = test_data\n",
        "        label = label.numpy()\n",
        "        if torch.cuda.is_available():\n",
        "            img = img.cuda()\n",
        "        else:\n",
        "            img = img.cpu()\n",
        "        img = torch.cat((img, img, img), 1)  # MNIST channel 1 -> 3\n",
        "        source_label_list.append(label)\n",
        "        source_img_list.append(img)\n",
        "\n",
        "    source_img_list = torch.stack(source_img_list)\n",
        "    source_img_list = source_img_list.view(-1, 3, 28, 28)\n",
        "\n",
        "    # Récupère échantillons target_test\n",
        "    target_label_list = []\n",
        "    target_img_list = []\n",
        "    for i, test_data in enumerate(target_test_loader):\n",
        "        if i >= 16:\n",
        "            break\n",
        "        img, label = test_data\n",
        "        label = label.numpy()\n",
        "        if torch.cuda.is_available():\n",
        "            img = img.cuda()\n",
        "        else:\n",
        "            img = img.cpu()\n",
        "        target_label_list.append(label)\n",
        "        target_img_list.append(img)\n",
        "\n",
        "    target_img_list = torch.stack(target_img_list)\n",
        "    target_img_list = target_img_list.view(-1, 3, 28, 28)\n",
        "\n",
        "    # Combine source_list + target_list\n",
        "    combined_label_list = source_label_list\n",
        "    combined_label_list.extend(target_label_list)\n",
        "    combined_img_list = torch.cat((source_img_list, target_img_list), 0)\n",
        "\n",
        "    source_domain_list = torch.zeros(512).type(torch.LongTensor)\n",
        "    target_domain_list = torch.ones(512).type(torch.LongTensor)\n",
        "    if torch.cuda.is_available():\n",
        "        combined_domain_list = torch.cat((source_domain_list, target_domain_list), 0).cuda()\n",
        "    else:\n",
        "        combined_domain_list = torch.cat((source_domain_list, target_domain_list), 0).cpu()\n",
        "\n",
        "    print(\"Extracting features to draw t-SNE plot...\")\n",
        "    combined_feature = encoder(combined_img_list)  # combined_feature : 1024,2352\n",
        "\n",
        "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=3000)\n",
        "    dann_tsne = tsne.fit_transform(combined_feature.detach().cpu().numpy())\n",
        "\n",
        "    print('Drawing t-SNE plot ...')\n",
        "    plot_embedding(dann_tsne, combined_label_list, combined_domain_list, training_mode)\n",
        "\n",
        "def process_data(data, expand_channels=False):\n",
        "    images, labels = data\n",
        "    if torch.cuda.is_available():\n",
        "        images, labels = images.cuda(), labels.cuda()\n",
        "    else:\n",
        "        images, labels = images.cpu(), labels.cpu()\n",
        "    if expand_channels:\n",
        "        images = images.repeat(1, 3, 1, 1)  # Répète les canaux pour convertir en images à 3 canaux\n",
        "    return images, labels\n",
        "\n",
        "def compute_output(encoder, classifier, images, alpha=None):\n",
        "    features = encoder(images)\n",
        "    if isinstance(classifier, Discriminator):\n",
        "        outputs = classifier(features, alpha)  # Classificateur de domaine\n",
        "    else:\n",
        "        outputs = classifier(features)  # Classificateur de labels\n",
        "    preds = outputs.data.max(1, keepdim=True)[1]\n",
        "    return preds\n",
        "\n",
        "def calculate_accuracy(correct, total):\n",
        "    return 100. * correct / total\n",
        "\n",
        "def print_accuracy(training_mode, accuracies):\n",
        "    print(f\"Test Results on {training_mode}:\")\n",
        "    for key, value in accuracies.items():\n",
        "        print(f\"{key} Accuracy: {value['correct']}/{value['total']} ({value['accuracy']:.2f}%)\")\n",
        "\n",
        "def tester(encoder, classifier, discriminator, source_test_loader, target_test_loader, training_mode):\n",
        "    if torch.cuda.is_available():\n",
        "        encoder.cuda()\n",
        "        classifier.cuda()\n",
        "    else:\n",
        "        encoder.cpu()\n",
        "        classifier.cpu()\n",
        "    set_model_mode('eval', [encoder, classifier])\n",
        "\n",
        "    if training_mode == 'DANN':\n",
        "        if torch.cuda.is_available():\n",
        "            discriminator.cuda()\n",
        "        else:\n",
        "            discriminator.cpu()\n",
        "        set_model_mode('eval', [discriminator])\n",
        "        domain_correct = 0\n",
        "\n",
        "    source_correct = 0\n",
        "    target_correct = 0\n",
        "\n",
        "    for batch_idx, (source_data, target_data) in enumerate(zip(source_test_loader, target_test_loader)):\n",
        "        p = float(batch_idx) / len(source_test_loader)\n",
        "        alpha = 2. / (1. + np.exp(-10 * p)) - 1 # formule donnée page 21 du papier [1]\n",
        "\n",
        "        # Traite les données source et cible\n",
        "        source_image, source_label = process_data(source_data, expand_channels=True)\n",
        "        target_image, target_label = process_data(target_data)\n",
        "\n",
        "        # Prédictions source et cible\n",
        "        source_pred = compute_output(encoder, classifier, source_image, alpha=None)\n",
        "        target_pred = compute_output(encoder, classifier, target_image, alpha=None)\n",
        "\n",
        "        # Mise à jour\n",
        "        source_correct += source_pred.eq(source_label.data.view_as(source_pred)).sum().item()\n",
        "        target_correct += target_pred.eq(target_label.data.view_as(target_pred)).sum().item()\n",
        "\n",
        "        if training_mode == 'DANN':\n",
        "            # Traite les images combinées pour la classification de domaine\n",
        "            combined_image = torch.cat((source_image, target_image), 0)\n",
        "            if torch.cuda.is_available():\n",
        "                domain_labels = torch.cat((torch.zeros(source_label.size(0), dtype=torch.long),\n",
        "                                       torch.ones(target_label.size(0), dtype=torch.long)), 0).cuda()\n",
        "            else:\n",
        "                domain_labels = torch.cat((torch.zeros(source_label.size(0), dtype=torch.long),\n",
        "                                           torch.ones(target_label.size(0), dtype=torch.long)), 0).cpu()\n",
        "\n",
        "            # Predictions de domaine\n",
        "            domain_pred = compute_output(encoder, discriminator, combined_image, alpha=alpha)\n",
        "            domain_correct += domain_pred.eq(domain_labels.data.view_as(domain_pred)).sum().item()\n",
        "\n",
        "    source_dataset_len = len(source_test_loader.dataset)\n",
        "    target_dataset_len = len(target_test_loader.dataset)\n",
        "\n",
        "    accuracies = {\n",
        "        \"Source\": {\n",
        "            \"correct\": source_correct,\n",
        "            \"total\": source_dataset_len,\n",
        "            \"accuracy\": calculate_accuracy(source_correct, source_dataset_len)\n",
        "        },\n",
        "        \"Target\": {\n",
        "            \"correct\": target_correct,\n",
        "            \"total\": target_dataset_len,\n",
        "            \"accuracy\": calculate_accuracy(target_correct, target_dataset_len)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if training_mode == 'DANN':\n",
        "        accuracies[\"Domain\"] = {\n",
        "            \"correct\": domain_correct,\n",
        "            \"total\": source_dataset_len + target_dataset_len,\n",
        "            \"accuracy\": calculate_accuracy(domain_correct, source_dataset_len + target_dataset_len)\n",
        "        }\n",
        "\n",
        "    print_accuracy(training_mode, accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeSboxnROgci"
      },
      "source": [
        "## 1. Source only (pas de DANN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "851L0lRROgci"
      },
      "source": [
        "*Penser à exécuter la cellule cachée au dessus pour avoir les fonction utiles pour la suite.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjAAQEGfXqLS"
      },
      "source": [
        "Le premier test que nous allons lancer avec nos données MNIST et MNIST-M est un réseau de neurones classique : un encodeur et un classificateur. Il sera entrainé sur le dataset MNIST (domaine source) uniquement, sans tenir compte du domaine cible (aucune branche de régresseur de domaine incluse dans le réseau). Nous allons ensuite vérifier son efficacité sur les nouvelles données MNIST-M (domaine cible)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9pQmIhsOgcj"
      },
      "outputs": [],
      "source": [
        "def source_only(encoder, classifier, source_train_loader, target_train_loader):\n",
        "    print(\"Training with only the source dataset\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        classifier_criterion = nn.CrossEntropyLoss().cuda()\n",
        "    else:\n",
        "        classifier_criterion = nn.CrossEntropyLoss().cpu()\n",
        "    optimizer = optim.SGD(\n",
        "        list(encoder.parameters()) +\n",
        "        list(classifier.parameters()),\n",
        "        lr=0.01, momentum=0.9)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "        set_model_mode('train', [encoder, classifier])\n",
        "\n",
        "        start_steps = epoch * len(source_train_loader)\n",
        "        total_steps = epochs * len(target_train_loader)\n",
        "\n",
        "        for batch_idx, (source_data, target_data) in enumerate(zip(source_train_loader, target_train_loader)):\n",
        "            source_image, source_label = source_data\n",
        "            p = float(batch_idx + start_steps) / total_steps\n",
        "\n",
        "            source_image = torch.cat((source_image, source_image, source_image), 1)  # MNIST conversion 3 canaux\n",
        "            if torch.cuda.is_available():\n",
        "                source_image, source_label = source_image.cuda(), source_label.cuda()  # 32\n",
        "            else:\n",
        "                source_image, source_label = source_image.cpu(), source_label.cpu()\n",
        "\n",
        "            optimizer = optimizer_scheduler(optimizer=optimizer, p=p)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            source_feature = encoder(source_image)\n",
        "\n",
        "            # Classification loss\n",
        "            class_pred = classifier(source_feature)\n",
        "            class_loss = classifier_criterion(class_pred, source_label)\n",
        "\n",
        "            class_loss.backward()\n",
        "            optimizer.step()\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                total_processed = batch_idx * len(source_image)\n",
        "                total_dataset = len(source_train_loader.dataset)\n",
        "                percentage_completed = 100. * batch_idx / len(source_train_loader)\n",
        "                print(f'[{total_processed}/{total_dataset} ({percentage_completed:.0f}%)]\\tClassification Loss: {class_loss.item():.4f}')\n",
        "\n",
        "        tester(encoder, classifier, None, source_test_loader, target_test_loader, training_mode='Source_only')\n",
        "\n",
        "    save_model(encoder, classifier, None, 'Source-only')\n",
        "    visualize(encoder, 'Source-only')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0XSTq6oXqLS"
      },
      "source": [
        "Vous êtes maintenant prêts à lancer le réseau de neurones !\n",
        "\n",
        "*Remarques* : la cellule suivante devrait mettre environ 5 minutes en utilisant un GPU (~ 8 minutes avec CPU de Google Colab). C'est donc le moment parfait pour aller prendre un café avant d'attaquer la partie visualisation et interprétation qui suit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1629
        },
        "id": "72nyKFPYOgcj",
        "outputId": "9bbbbfcb-965d-414d-946d-78eeb4d83e2b"
      },
      "outputs": [],
      "source": [
        "source_only(encoder, classifier, source_train_loader, target_train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poZ73UO0XqLT"
      },
      "source": [
        "Pour visualiser, nous avons choisi le **diagramme t-SNE (t-distributed Stochastic Neighbor Embedding)**. Il s'agit d'une méthode de réduction de dimensionnalité utilisée pour visualiser des données de grande dimension. Elle fonctionne en préservant la structure locale des données, assurant que les points proches dans l'espace d'origine restent proches dans l'espace réduit.\n",
        "\n",
        "L'exemple que vous avez fait tourner ne comportait que 3 époques pour des raisons évidentes de temps de calcul. Vous trouverez juste après le résultat pour 10 époques (~ 16 minutes de calcul avec GPU), avec l'image *background.jpg*.\n",
        "\n",
        "Votre résultat peut être légèrement différent du fait du nombre d'époques mais voilà une idée (en mieux !) de ce que vous devriez obtenir :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08-FxsQ_XqLT"
      },
      "source": [
        " <details class=\"alert alert-success\">\n",
        "    <summary markdown=\"span\"><b>Voir la TSNE - source only - 10 époques (cliquer ici)</b></summary>\n",
        "\n",
        " ![TSNE source only - 10 epochs](./img/plot_source_only_10_epochs.png)\n",
        " ![résultats source only - 10 epochs](./img/res_source_only_10_epochs.png)\n",
        "\n",
        "Légende :\n",
        "- chiffres bleus : domaine source (MNIST);\n",
        "- chiffres rouges : domaine cible (MNIST-M).\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnehKKiKXqLT"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "<b>Exercice 5</b> : Quelle est votre première interprétation en voyant cela ?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuK1lLMlXqLT"
      },
      "source": [
        " <details class=\"alert alert-success\">\n",
        "    <summary markdown=\"span\"><b>Interprétation (cliquer ici)</b></summary>\n",
        "\n",
        "1. Si l'on regarde d'abord les <b>chiffres bleus</b> : ils correspondent au test du NN avec le dataset source MNIST, autrement dit celui pour lequel il a été entraîné. On remarque la formation de paquets bien nets et bien distincts entre les classes de chiffres, avec quelques petites erreurs mais tout de même une très bonne précision (~92%). Cela signifie que le NN a bien été entrainé, jusque là tout est normal !\n",
        "2. Si l'on regarde maintenant les <b>chiffres rouges</b> : ça devient plus compliqué ... ils correspondent au test du NN avec le dataset cible MNIST-M qui présente un décalage dans sa distribution comparé aux données d'entrainement MNIST. On voit que la classification est très limitée, tous les chiffres sont regroupés en un gros paquet désordonné. L'algorithme classifie en prenant compte du domaine (excatement ce qu'on ne veut pas). Notons qu'il y a quand même quelques réussites mais la précision est largement réduite (~56%).\n",
        "\n",
        "</details>\n",
        "\n",
        "La solution pour répondre à ce problème est évidemment le DANN ! Voyons s'il y a des améliorations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YAHKpQGOgcj"
      },
      "source": [
        "## 2. DANN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iYKRvOBXqLT"
      },
      "source": [
        "A la différence du code du dessus, on ajoute un discriminateur (ou régresseur de domaine) pour la classification de domaine associé à une GRL pour en faire un DANN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc4AER5BXqLT"
      },
      "outputs": [],
      "source": [
        "def dann(encoder, classifier, discriminator, source_train_loader, target_train_loader):\n",
        "    print(\"Training with the DANN adaptation method\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      classifier_criterion = nn.CrossEntropyLoss().cuda()\n",
        "      discriminator_criterion = nn.CrossEntropyLoss().cuda()\n",
        "    else:\n",
        "      classifier_criterion = nn.CrossEntropyLoss().cpu()\n",
        "      discriminator_criterion = nn.CrossEntropyLoss().cpu()\n",
        "\n",
        "    optimizer = optim.SGD(\n",
        "        list(encoder.parameters()) +\n",
        "        list(classifier.parameters()) +\n",
        "        list(discriminator.parameters()),\n",
        "        lr=0.01,\n",
        "        momentum=0.9)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "        set_model_mode('train', [encoder, classifier, discriminator])\n",
        "\n",
        "        start_steps = epoch * len(source_train_loader)\n",
        "        total_steps = epochs * len(target_train_loader)\n",
        "\n",
        "        for batch_idx, (source_data, target_data) in enumerate(zip(source_train_loader, target_train_loader)):\n",
        "\n",
        "            source_image, source_label = source_data\n",
        "            target_image, target_label = target_data\n",
        "\n",
        "            p = float(batch_idx + start_steps) / total_steps\n",
        "            alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "\n",
        "            source_image = torch.cat((source_image, source_image, source_image), 1)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "              source_image, source_label = source_image.cuda(), source_label.cuda()\n",
        "              target_image, target_label = target_image.cuda(), target_label.cuda()\n",
        "            else:\n",
        "              source_image, source_label = source_image.cpu(), source_label.cpu()\n",
        "              target_image, target_label = target_image.cpu(), target_label.cpu()\n",
        "            combined_image = torch.cat((source_image, target_image), 0)\n",
        "\n",
        "            optimizer = optimizer_scheduler(optimizer=optimizer, p=p)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            combined_feature = encoder(combined_image)\n",
        "            source_feature = encoder(source_image)\n",
        "\n",
        "            # 1.Classification loss\n",
        "            class_pred = classifier(source_feature)\n",
        "            class_loss = classifier_criterion(class_pred, source_label)\n",
        "\n",
        "            # 2. Domain loss\n",
        "            domain_pred = discriminator(combined_feature, alpha)\n",
        "\n",
        "            domain_source_labels = torch.zeros(source_label.shape[0]).type(torch.LongTensor)\n",
        "            domain_target_labels = torch.ones(target_label.shape[0]).type(torch.LongTensor)\n",
        "            if torch.cuda.is_available():\n",
        "              domain_combined_label = torch.cat((domain_source_labels, domain_target_labels), 0).cuda()\n",
        "            else:\n",
        "              domain_combined_label = torch.cat((domain_source_labels, domain_target_labels), 0).cpu()\n",
        "            domain_loss = discriminator_criterion(domain_pred, domain_combined_label)\n",
        "\n",
        "            total_loss = class_loss + domain_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print('[{}/{} ({:.0f}%)]\\tTotal Loss: {:.4f}\\tClassification Loss: {:.4f}\\tDomain Loss: {:.4f}'.format(\n",
        "                    batch_idx * len(target_image), len(target_train_loader.dataset), 100. * batch_idx / len(target_train_loader), total_loss.item(), class_loss.item(), domain_loss.item()))\n",
        "\n",
        "        tester(encoder, classifier, discriminator, source_test_loader, target_test_loader, training_mode='DANN')\n",
        "\n",
        "    save_model(encoder, classifier, discriminator, 'DANN')\n",
        "    visualize(encoder, 'DANN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsqiF01SXqLT"
      },
      "source": [
        "De même que précédemment, la cellule suivante met environ 5 minutes avec GPU à tourner (~ 15 minutes avec CPU de Google Colab) --> un autre café ?\n",
        "\n",
        "*Remarque :* pour des résultats plus marqués, il est possible d'augmenter le nombre d'époques, mais cela prendra forcément plus de temps. Voir résultats donnés juste après."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1736
        },
        "id": "yVwm_6MaOgcj",
        "outputId": "6fc3e7bb-9d41-4056-bec6-ccbdbecf9087"
      },
      "outputs": [],
      "source": [
        "dann(encoder, classifier, discriminator, source_train_loader, target_train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq8wvDPxXqLU"
      },
      "source": [
        " Il vous est une nouvelle fois proposé une visualisation TSNE avec 10 époques (~ 17 minutes de calcul avec GPU) pour l'image *background.jpg* :\n",
        "\n",
        " <details class=\"alert alert-success\">\n",
        "    <summary markdown=\"span\"><b>Voir la TSNE - DANN - 10 époques (cliquer ici)</b></summary>\n",
        "\n",
        " ![TSNE DANN - 10 epochs](./img/plot_dann_10_epochs.png)\n",
        " ![résultats DANN - 10 epochs](./img/res_dann_10_epochs.png)\n",
        "\n",
        "Légende :\n",
        "- chiffres bleus : domaine source (MNIST);\n",
        "- chiffres rouges : domaine cible (MNIST-M).\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmdeeQiSXqLU"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "<b>Exercice 6</b> : Comparez et interprétez ces nouveaux résultats.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81w35nX6XqLU"
      },
      "source": [
        " <details class=\"alert alert-success\">\n",
        "    <summary markdown=\"span\"><b>Interprétation (cliquer ici)</b></summary>\n",
        "\n",
        "1. Si l'on regarde d'abord les <b>chiffres bleus</b> : encore une fois tout est normal ! Les chiffres sont agglomérés en fonction de leur classe et sont bien prédits avec une précision de 91% (le NN fait bien son travail), bien qu'on note une très légère baisse comparé à l'exemple *source-only*.\n",
        "2. Si l'on regarde maintenant les <b>chiffres rouges </b>: on remarque une nette différence avec le cas précédent ! Déjà, la précision a largement été améliorée avec un score de presque 87%, ce qui est loin des 57% de tout à l'heure. L'efficacité du DANN sur ce genre de problème est alors incontestable. On voit en plus que les chiffres rouges (MNIST-M) sont répartis et éparpillés de manière plus homogène au sein des bleus (MNIST). C'est de là que vient le score plus élevé : l'algorithme ne sait plus faire la différence entre le domaine source et le domaine cible (score de moins de 57%), il se trompe près d'une fois sur deux. Finalement, cette méthode permet clairement de faire des prédictions selon la distribution des caractéristiques et non pas du domaine.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Au final, voici les résultats résumés au sein d'un tableau, pour l'image 1 (*background.jpg*) et l'image 2 (*background2.jpg*), avec epochs=10, batch_size=32 et num_workers=4.\n",
        "\n",
        "![Tableau résultats DANN](./img/table_res_DANN.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-zBlmtvOgck"
      },
      "source": [
        "# <a id=\"sec7\">7. Autres résultats</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPW5SasqXqLU"
      },
      "source": [
        "### Données synthétiques et réelles\n",
        "\n",
        "Pour compléter l'expérience ci-dessus, il est possible de la comparer avec les résultats de l'article [Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818) [1].\n",
        "Les conditions étaient les suivantes :\n",
        "- batch-size = 128 ;\n",
        "- pas d'information sur le nombre d'époques (une centaine ? plus ? beaucoup ...) ;\n",
        "- pour plus de détails, cf pages 20-23 de [1].\n",
        "\n",
        "![Paires à tester](./img/pairs_to_test.png)\n",
        "\n",
        "1. *MNIST -> MNIST-M* : pour obtenir le domaine cible (MNIST-M), mélange des chiffres de l'ensemble d'origine MNIST sur des patchs extraits de manière aléatoire à partir de photos couleur du BSDS500 (Arbelaez et al., 2011) [12] ;\n",
        "2. *Nombres synthétiques -> SVHN* (*Street-View House Number*, Netzer et al., 2011) [13] : scénario courant d'entraînement sur des données synthétiques et de test sur des données réelles.\n",
        "\n",
        "![Résultats paires à tester](./img/res_pairs_to_test.png)\n",
        "\n",
        "**Interprétation :** Encore une fois, on voit que l’adaptation rend les deux distributions de caractéristiques beaucoup plus proches.\n",
        "\n",
        "![Table paires à tester](./img/res_pairs_to_test_table.png)\n",
        "\n",
        "**Interprétation :** La première ligne correspond à la limite inférieure de performance (c'est-à-dire, si aucune adaptation n'est réalisée). La dernière ligne correspond à l'entraînement sur les données de domaine cible si l'on avait les labels connus (limite supérieure). Pour les deux cas, l'approche DANN surpasse considérablement la méthode SA (*subspace alignment*) de Fernando et al. [14] et couvre même une grande partie de l'écart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Autres datasets\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "Le <b> dataset OFFICE </b> (Saenko et al., 2010) [12] est une collection d'images de produits de bureau utilisée dans la recherche sur l'adaptation de domaine, qui comprend des images provenant de trois sources distinctes : <b> Amazon, une webcam et un appareil photo DSLR </b>. Ces images représentent les mêmes catégories d'objets mais diffèrent par leur qualité et leur contexte en raison des différentes sources. Le dataset est conçu pour développer des modèles de machine learning qui peuvent s'adapter et généraliser d'un domaine source vers un domaine cible sans données étiquetées supplémentaires pour ce dernier (adaptation de domaine non-supervisée).\n",
        "</div>\n",
        "\n",
        "![Résultats OFFICE dataset](./img/res_office.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN5mNUhLOgck"
      },
      "source": [
        "# <a id=\"sec8\">8. Ouverture à d'autres applications </a>\n",
        "\n",
        "Jusqu'ici, nous avons implémenté un exemple concret de DANN dans le contexte de la **classification d'images**.\n",
        "\n",
        "\n",
        "### Analyse de sentiments\n",
        "\n",
        "Une autre application reconnue du DANN est **l'analyse de sentiments**. Le DANN améliore par exemple le state-of-the-art *marginalized Stacked Autoencoders* (mSDA) de Chen et al. (2012) [9] sur le benchmark commun des avis Amazon.\n",
        "\n",
        "### Ré-identification de personnes\n",
        "\n",
        "Le DANN a également été validé dans le cadre d'une application de **ré-identification de personnes** (Gong et al., 2014) [16], où le but est de reconnaître ou de suivre des individus à travers différentes caméras ou dans des images différentes. Elle est utilisée dans les systèmes de surveillance et de sécurité pour suivre les mouvements des personnes dans différents emplacements ou à différents moments. Des points de vue disjoints, des poses différentes, mais aussi une luminosité et une qualité différentes rendent la tâche extrêmement compliquée, même pour les humains.\n",
        "La ré-identification de personnes entre bien dans le cadre de l'adaptation de domaine puisque chaque réseau de caméras est en fait un domaine différent.\n",
        "\n",
        "![Ré-identification de personnes](./img/re_id.png)\n",
        "\n",
        "Les méthodes de deep learning ne sont pas forcément les meilleures dans ce domaine (sûrement à cause des datasets d'entrainement de taille limitée) mais le DANN représente une bonne voie d'amélioration en ré-identification.\n",
        "\n",
        "![Résultats ré-identification de personnes](./img/res_re_id.png)\n",
        "\n",
        "Cette application sort un peu du cadre de notre étude et se présente comme une ouverture à l'étendue du champ d'application du DANN. Pour plus d'informations, se référer aux pages 26-30 de l'article [1].\n",
        "\n",
        "### Applications plus récentes\n",
        "\n",
        "Depuis la publicatoin de l'article [1] en 2015, de nouvelles applications on été testées et approuvées.\n",
        "Par exemple :\n",
        "- la reconnaissance automatique de la parole avec différents accents (*Domain Adversarial Training for Accented Speech Recognition, Sun et al., 2017*) [17] ;\n",
        "- l'application en imagerie médicale pour la détection de mitoses dans les images de cancer du sein ou pour la segmentation des lésions cérébrales (*Domain-Adversarial Neural Networks to Address the Appearance Variability of Histopathology Images, Lafarge et al., 2017* et *Unsupervised Domain Adaptation in Brain Lesion Segmentation with Adversarial Networks, Kamnitsas et al., 2016*) [18, 19] ;\n",
        "- l'étude de l'influence des descriptions de produits dans le commerce électronique sur les comportements d'achat des consommateurs (*Predicting Sales from the Language of Product Descriptions, Pryzant et al., 2017*) [20] ;\n",
        "- l'identification des transitions de phase dans divers modèles quantiques (*Adversarial Domain Adaptation for Identifying Phase Transitions, Huembeli et al., 2017*) [21] ;\n",
        "- l'amélioration de la saisie en robotique (*Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping, Bousmalis et al., 2017*) [22] ;\n",
        "- etc.\n",
        "\n",
        "Les champs d'application du DANN sont extrêmement vastes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYErOd1hOgck"
      },
      "source": [
        "# <a id=\"sec9\">9. Conclusion</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAsu7oTjOgck"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "* <b>L'objectif</b> de ce cours était de présenter une nouvelle approche d’apprentissage pour l'<b>adaptation de domaine (DA)</b> en favorisant l'émergence de représentations qui sont :\n",
        "\n",
        "        1. discriminantes pour la tâche d'apprentissage principale sur le domaine source,\n",
        "        2. non-discriminantes pour la détection du domaine.\n",
        "        \n",
        "* <b>Les moyens</b> : proposer une <b>approche simple et générique</b> d’apprentissage d'adaptation de domaine, qui peut être implémentée avec peu d’effort en utilisant n’importe quel paquet de deep learning, et qui offre une interprétation théorique basée sur la $H$-divergence.\n",
        "* <b>La méthode</b> : utiliser un <b>réseau de neurones profond</b> avec une <b>couche spéciale de renversement du gradient (GRL)</b>, qui permet d’optimiser les caractéristiques de manière à minimiser la perte de prédiction des labels sur le domaine source et à maximiser la perte de classification du domaine sur les deux domaines.\n",
        "* <b>Résultats</b> : le DANN nous a permis d'atteindre des performances de pointe sur plusieurs tâches d’adaptation de domaine, telles que <b>la classification d’images, l’analyse de sentiments et la ré-identification de personnes</b>, en utilisant différentes architectures de réseaux de neurones.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jibcJl5XqLV"
      },
      "source": [
        "# <a id=\"sec10\">10. Références</a>\n",
        "Le contenu de ce notebook repose en grande partie sur la lecture de [1]. D'autres articles et supports sont venus le compléter.\n",
        "\n",
        "1. Ganin Y., Ustinova E., Ajakan H., Germain P., Larochelle H., Laviolette F., Marchand M., and Lempitsky V. [Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818). In *Journal of Machine Learning Research*, pages 1-35, 2015.\n",
        "2. Farahani A., Voghoei S., Rasheed K., and R. Arabnia H. [A Brief Review Of Domain Adaptation](https://arxiv.org/pdf/2010.03978v1.pdf), 2020.\n",
        "3. Github [NaJaeMin92/pytorch-DANN] (https://github.com/NaJaeMin92/pytorch-DANN/tree/master)\n",
        "4. Github [vcoyette/DANN](https://github.com/vcoyette/DANN/tree/master)\n",
        "5. Vidéo YouTube [Introduction aux Domain Adversarial Neural Networks (DANN)](https://www.youtube.com/watch?v=_Y1GAt3UxO4) par Philippe Giguère, Université Laval, 2023.\n",
        "6. Vidéo YouTube [[ML 2021 (English version)] Lecture 27: Domain Adaptation](https://www.youtube.com/watch?v=8AKqH6V9kjE) par Hung-yi Lee, 2021.\n",
        "7. Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. In *NIPS*, pages 137- 144, 2006.\n",
        "8. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. *Machine Learning*, 79(1-2):151-175, 2010.\n",
        "9. Minmin Chen, Zhixiang Eddie Xu, Kilian Q. Weinberger, and Fei Sha. Marginalized denoising autoencoders for domain adaptation. In *ICML*, pages 767-774, 2012.\n",
        "10. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. *Proceedings of the IEEE*, 86(11):2278-2324, November 1998.\n",
        "11. Erheng Zhong, Wei Fan, Qiang Yang, Olivier Verscheure, and Jiangtao Ren. Cross validation framework to choose amongst models and datasets for transfer learning. In *Machine Learning and Knowledge Discovery in Databases*, pages 547-562. Springer, 2010.\n",
        "12. Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and hierarchical image segmentation. *IEEE Transaction Pattern Analysis and Machine Intelligence*, 33, 2011.\n",
        "13. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In *NIPS Workshop on Deep Learning and Unsupervised Feature Learning*, 2011.\n",
        "14. Basura Fernando, Amaury Habrard, Marc Sebban, and Tinne Tuytelaars. Unsupervised visual domain adaptation using subspace alignment. In *ICCV*, 2013.\n",
        "15. Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In *ECCV*, pages 213-226, 2010.\n",
        "16. Shaogang Gong, Marco Cristani, Shuicheng Yan, and Chen Change Loy. Person re-identiffication. *Springer*, 2014.\n",
        "17. S. Sun, C.-F. Yeh, M.-Y. Hwang, M. Ostendorf, and L. Xie. Domain Adversarial Training for Accented Speech Recognition. 2017.\n",
        "18. M. W. Lafarge, J. P. W. Pluim, K. A. J. Eppenhof, P. Moeskops, and M. Veta. Domain-Adversarial Neural Networks to Address the Appearance Variability of Histopathology Images. 2017.\n",
        "19. K. Kamnitsas, C. Baumgartner, Ch. Ledig, V. F.J. Newcombe, J. P. Simpson, A. D. Kane, D. K. Menon, A. Nori, A. Criminisi, D. Rueckert, B. Glocker. Unsupervised Domain Adaptation in Brain Lesion Segmentation with Adversarial Networks. 2016.\n",
        "20. R. Pryzant, Y. Chung, and D. Jurafsky. Predicting Sales from the Language of Product Descriptions. 2017.\n",
        "21. P. Huembeli, A. Dauphin, and P. Wittek. Adversarial Domain Adaptation for Identifying Phase Transitions. 2017.\n",
        "22. K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, S. Levine, V. Vanhoucke. Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping. 2017."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axcBdURbXqLV"
      },
      "source": [
        "# --- FIN ---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
